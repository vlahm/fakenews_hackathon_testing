setwd('~/git/fakenews/')
library(rvest)
fake = read.csv('fake.csv')
head(fake)
names(fake)
str(fake)
fake = read.csv('fake.csv', stringsAsFactors=FALSE)
head(fake)
names(fake)
str(fake)
length(unique(fake$author))
unique(fake$author)
any(is.na(unique(fake$author)))
str(fake)
fake = read.csv('fake.csv', stringsAsFactors=FALSE)
names(fake)
fake$text[1]
View(fake)
nchar(fake$text[1])_
nchar(fake$text[1])
max(nchar(fake$text))
nchar(fake$text[1:10])
library(caret)
if(!require(caret))install.packages('caret')
names(fake)
head(fake)
if(!require(NLP))install.packages('NLP')
install.packages("cleanNLP")
library(cleanNLP)
init_spaCy()
if(!require(reticulate))install.packages('reticulate')
names(fake)
unique(fake$type)
length(which(fake$type=='bs'))
length(which(fake$type=='bs'))/12999
length(which(fake$type=='fake'))/12999
length(which(fake$type=='satire'))/12999
table(fake$type)
all = read.csv('uci-news-aggregator.csv', stringsAsFactors=FALSE)
names(all)
unique(all$CATEGORY)
unique(all$PUBLISHER)
names(all)
head(all)
View(all)
req = "curl --get --include 'https://webhose.io/filterWebContent?token=3cd317bf-5cd9-4ebc-b5a8-c180890936f0&format=json&q=site%3Acnn.com' \
-H 'Accept: text/plain'"
resp <- make_req(straighten(req))
library(curlconverter)
install.packages('curlconverter')
